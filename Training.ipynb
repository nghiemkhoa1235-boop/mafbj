{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPy8JqqoNyNKZlLiqaFKHX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nghiemkhoa1235-boop/mafbj/blob/main/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzgB23znbuQ-"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# üöÄ ULTIMATE FOOD RECOGNITION TRAINING\n",
        "# ===============================\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.optimizers import Adam, AdamW\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, CSVLogger, LearningRateScheduler, TerminateOnNaN, LambdaCallback\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB4\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import cv2\n",
        "from google.colab import files, drive\n",
        "import warnings\n",
        "import json\n",
        "import glob\n",
        "import shutil\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# üöÄ INITIAL SETUP & OPTIMIZATION\n",
        "# ===============================\n",
        "\n",
        "\n",
        "print(\"üöÄ STARTING ULTIMATE FOOD RECOGNITION TRAINING\")\n",
        "print(\"‚úÖ TensorFlow version:\", tf.__version__)\n",
        "print(\"‚úÖ GPU Available:\", tf.test.is_gpu_available())\n",
        "\n",
        "\n",
        "# T·ªëi ∆∞u h√≥a GPU\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "data_root = '/content/drive/MyDrive/AI/do_an'\n",
        "\n",
        "\n",
        "# T·∫°o th∆∞ m·ª•c backup tr√™n Drive n·∫øu ch∆∞a c√≥ (v·ªõi subfolders)\n",
        "backup_dir = os.path.join(data_root, 'training_backups')\n",
        "os.makedirs(backup_dir, exist_ok=True)\n",
        "os.makedirs(os.path.join(backup_dir, 'logs'), exist_ok=True)\n",
        "os.makedirs(os.path.join(backup_dir, 'stage1_checkpoints'), exist_ok=True)\n",
        "os.makedirs(os.path.join(backup_dir, 'stage2_checkpoints'), exist_ok=True)\n",
        "os.makedirs(os.path.join(backup_dir, 'evaluation'), exist_ok=True)\n",
        "print(f\"üìÅ Backup directory created/verified: {backup_dir}\")\n",
        "print(f\"   Subfolders: logs, stage1_checkpoints, stage2_checkpoints, evaluation\")\n",
        "\n",
        "\n",
        "# Test Drive write access\n",
        "test_file = os.path.join(backup_dir, 'test_write_access.txt')\n",
        "with open(test_file, 'w') as f:\n",
        "    f.write('Test successful - Drive OK!')\n",
        "print(f\"‚úÖ Drive write test: {test_file} created. Check Drive to confirm.\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# üîÑ RESUME TRAINING FUNCTIONALITY\n",
        "# ===============================\n",
        "\n",
        "\n",
        "def get_latest_checkpoint(checkpoint_pattern):\n",
        "    \"\"\"Find the latest checkpoint file based on epoch in filename.\"\"\"\n",
        "    checkpoints = glob.glob(checkpoint_pattern)\n",
        "    if not checkpoints:\n",
        "        return None, 0\n",
        "    print(f\"üîç Found local checkpoints: {[os.path.basename(c) for c in checkpoints]}\")\n",
        "    # Extract epoch from filename, assuming pattern like 'stage1_epoch_XX.keras'\n",
        "    epochs = []\n",
        "    for cp in checkpoints:\n",
        "        try:\n",
        "            epoch = int(''.join(filter(str.isdigit, cp.split('_')[-1].split('.')[0])))\n",
        "            epochs.append((cp, epoch))\n",
        "        except:\n",
        "            pass\n",
        "    if epochs:\n",
        "        latest = max(epochs, key=lambda x: x[1])\n",
        "        print(f\"‚úÖ Latest local checkpoint: {os.path.basename(latest[0])} (epoch {latest[1]})\")\n",
        "        return latest[0], latest[1]\n",
        "    return None, 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_training_state():\n",
        "    \"\"\"Load the current training state from JSON file.\"\"\"\n",
        "    state_file = 'training_state.json'\n",
        "    # ∆Øu ti√™n load t·ª´ Drive n·∫øu c√≥\n",
        "    drive_state = os.path.join(backup_dir, state_file)\n",
        "    if os.path.exists(drive_state):\n",
        "        shutil.copy(drive_state, state_file)\n",
        "        print(f\"üìÇ Loaded state from Drive: {drive_state}\")\n",
        "    if os.path.exists(state_file):\n",
        "        with open(state_file, 'r') as f:\n",
        "            return json.load(f)\n",
        "    print(\"‚ö†Ô∏è No state file found. Will infer from checkpoints if available.\")\n",
        "    return {\n",
        "        'current_stage': 0,\n",
        "        'stage1_epochs_trained': 0,\n",
        "        'stage2_epochs_trained': 0,\n",
        "        'best_val_acc_stage1': 0.0,\n",
        "        'model_loaded': False\n",
        "    }\n",
        "\n",
        "\n",
        "def save_training_state(stage, epochs_trained, best_val_acc=0.0):\n",
        "    \"\"\"Save the current training state to JSON file.\"\"\"\n",
        "    state = load_training_state()\n",
        "    if stage == 1:\n",
        "        state['current_stage'] = 1\n",
        "        state['stage1_epochs_trained'] = epochs_trained\n",
        "        state['best_val_acc_stage1'] = best_val_acc\n",
        "    elif stage == 2:\n",
        "        state['current_stage'] = 2\n",
        "        state['stage2_epochs_trained'] = epochs_trained\n",
        "    with open('training_state.json', 'w') as f:\n",
        "        json.dump(state, f)\n",
        "    # Copy to Drive\n",
        "    drive_state = os.path.join(backup_dir, 'training_state.json')\n",
        "    shutil.copy('training_state.json', drive_state)\n",
        "    print(f\"üíæ State saved to Drive: {drive_state} ‚úÖ\")\n",
        "\n",
        "\n",
        "def backup_file_to_drive(local_file, drive_subdir=''):\n",
        "    \"\"\"Backup a file to Drive backups directory with detailed log.\"\"\"\n",
        "    if not os.path.exists(local_file):\n",
        "        print(f\"‚ö†Ô∏è Local file not found: {local_file}\")\n",
        "        return\n",
        "    drive_path = os.path.join(backup_dir, drive_subdir, os.path.basename(local_file))\n",
        "    os.makedirs(os.path.dirname(drive_path), exist_ok=True)\n",
        "    try:\n",
        "        shutil.copy(local_file, drive_path)\n",
        "        if os.path.exists(drive_path):\n",
        "            print(f\"‚úÖ Backup success: {os.path.basename(local_file)} ‚Üí {drive_path}\")\n",
        "        # B·ªé CLEANUP - GI·ªÆ H·∫æT CHECKPOINTS\n",
        "        # checkpoint_folder = os.path.join(backup_dir, drive_subdir)\n",
        "        # cleanup_checkpoints(checkpoint_folder)\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Backup copied but file not found after: {drive_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Backup failed for {local_file}: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Callback ƒë·ªÉ backup checkpoint m·ªói epoch l√™n Drive\n",
        "def drive_backup_callback(stage_name):\n",
        "    def on_epoch_end(epoch, logs):\n",
        "        # Backup checkpoint hi·ªán t·∫°i\n",
        "        cp_pattern = f'{stage_name}_epoch_{epoch+1:02d}.keras'\n",
        "        if os.path.exists(cp_pattern):\n",
        "            backup_file_to_drive(cp_pattern, f'{stage_name}_checkpoints')\n",
        "        # Backup CSV log t·∫°m th·ªùi\n",
        "        csv_file = f'ultimate_{ \"training\" if stage_name == \"stage1\" else \"fine_tuning\" }_log.csv'\n",
        "        if os.path.exists(csv_file):\n",
        "            backup_file_to_drive(csv_file, 'logs')\n",
        "        # Backup state.json\n",
        "        current_epoch = epoch + 1\n",
        "        save_training_state(1 if stage_name == \"stage1\" else 2, current_epoch)\n",
        "        print(f\"üîÑ Epoch {current_epoch} backup completed to Drive!\")\n",
        "    return LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "\n",
        "def resume_stage1(model, csv_logger_file='ultimate_training_log.csv'):\n",
        "    \"\"\"Resume stage 1 training.\"\"\"\n",
        "    state = load_training_state()\n",
        "    print(f\"üîç Checking resume for Stage 1: {state['stage1_epochs_trained']} epochs already trained\")\n",
        "    if state['stage1_epochs_trained'] > 0:\n",
        "        print(f\"üìÇ Resuming Stage 1 from epoch {state['stage1_epochs_trained'] + 1}\")\n",
        "        # Load latest checkpoint from local or Drive\n",
        "        cp_file, cp_epoch = get_latest_checkpoint('stage1_epoch_*.keras')\n",
        "        if not cp_file:\n",
        "            # Try from Drive\n",
        "            drive_cp_pattern = os.path.join(backup_dir, 'stage1_checkpoints', 'stage1_epoch_*.keras')\n",
        "            drive_checkpoints = glob.glob(drive_cp_pattern)\n",
        "            if drive_checkpoints:\n",
        "                # Sort to get latest\n",
        "                drive_epochs = []\n",
        "                for cp in drive_checkpoints:\n",
        "                    try:\n",
        "                        epoch = int(''.join(filter(str.isdigit, os.path.basename(cp).split('_')[-1].split('.')[0])))\n",
        "                        drive_epochs.append((cp, epoch))\n",
        "                    except:\n",
        "                        pass\n",
        "                if drive_epochs:\n",
        "                    drive_epochs.sort(key=lambda x: x[1], reverse=True)\n",
        "                    cp_file, cp_epoch = drive_epochs[0]\n",
        "                    shutil.copy(cp_file, os.path.basename(cp_file))\n",
        "                    cp_file = os.path.basename(cp_file)\n",
        "                    print(f\"üìÇ Loaded checkpoint from Drive: {cp_file} (epoch {cp_epoch})\")\n",
        "        if cp_file:\n",
        "            model.load_weights(cp_file)\n",
        "            print(f\"‚úÖ Loaded weights from {cp_file} (epoch {cp_epoch})\")\n",
        "            state['model_loaded'] = True\n",
        "            # ∆Øu ti√™n checkpoint epoch n·∫øu m·ªõi h∆°n state\n",
        "            if cp_epoch > state['stage1_epochs_trained']:\n",
        "                print(f\"üîÑ Updating state from checkpoint: {state['stage1_epochs_trained']} ‚Üí {cp_epoch}\")\n",
        "                state['stage1_epochs_trained'] = cp_epoch\n",
        "                save_training_state(1, cp_epoch)\n",
        "        # Fallback to CSV only if no checkpoint\n",
        "        epochs_trained = state['stage1_epochs_trained']\n",
        "        if os.path.exists(csv_logger_file) and not cp_file:\n",
        "            # Try Drive first\n",
        "            drive_csv = os.path.join(backup_dir, 'logs', csv_logger_file)\n",
        "            if os.path.exists(drive_csv):\n",
        "                shutil.copy(drive_csv, csv_logger_file)\n",
        "                print(f\"üìÇ Loaded CSV log from Drive: {drive_csv}\")\n",
        "            df_log = pd.read_csv(csv_logger_file)\n",
        "            csv_epochs = len(df_log)\n",
        "            print(f\"üìä CSV log shows {csv_epochs} epochs trained\")\n",
        "            if csv_epochs > epochs_trained:\n",
        "                epochs_trained = csv_epochs\n",
        "                save_training_state(1, epochs_trained)\n",
        "        return epochs_trained\n",
        "    print(\"‚úÖ No previous training found for Stage 1. Starting from epoch 0.\")\n",
        "    return 0\n",
        "\n",
        "\n",
        "def resume_stage2(model, csv_logger_file='ultimate_fine_tuning_log.csv'):\n",
        "    \"\"\"Resume stage 2 training.\"\"\"\n",
        "    state = load_training_state()\n",
        "    print(f\"üîç Checking resume for Stage 2: {state['stage2_epochs_trained']} epochs already trained\")\n",
        "    if state['stage2_epochs_trained'] > 0:\n",
        "        print(f\"üìÇ Resuming Stage 2 from epoch {state['stage2_epochs_trained'] + 1}\")\n",
        "        # Load latest checkpoint from local or Drive\n",
        "        cp_file, epoch_offset = get_latest_checkpoint('stage2_epoch_*.keras')\n",
        "        if not cp_file:\n",
        "            # Try from Drive\n",
        "            drive_cp_pattern = os.path.join(backup_dir, 'stage2_checkpoints', 'stage2_epoch_*.keras')\n",
        "            drive_checkpoints = glob.glob(drive_cp_pattern)\n",
        "            if drive_checkpoints:\n",
        "                # Sort to get latest\n",
        "                drive_checkpoints.sort(key=lambda x: int(''.join(filter(str.isdigit, os.path.basename(x).split('_')[-1].split('.')[0]))))\n",
        "                cp_file = drive_checkpoints[-1]\n",
        "                shutil.copy(cp_file, os.path.basename(cp_file))\n",
        "                cp_file = os.path.basename(cp_file)\n",
        "                print(f\"üìÇ Loaded checkpoint from Drive: {cp_file}\")\n",
        "        if cp_file:\n",
        "            model.load_weights(cp_file)\n",
        "            print(f\"‚úÖ Loaded weights from {cp_file}\")\n",
        "            state['model_loaded'] = True\n",
        "        # Determine additional epochs from CSV log\n",
        "        if os.path.exists(csv_logger_file):\n",
        "            # Try Drive first\n",
        "            drive_csv = os.path.join(backup_dir, 'logs', csv_logger_file)\n",
        "            if os.path.exists(drive_csv):\n",
        "                shutil.copy(drive_csv, csv_logger_file)\n",
        "                print(f\"üìÇ Loaded CSV log from Drive: {drive_csv}\")\n",
        "            df_log = pd.read_csv(csv_logger_file)\n",
        "            additional_epochs = len(df_log)\n",
        "            print(f\"üìä CSV log shows {additional_epochs} epochs for Stage 2\")\n",
        "            if additional_epochs > state['stage2_epochs_trained']:\n",
        "                state['stage2_epochs_trained'] = additional_epochs\n",
        "                save_training_state(2, additional_epochs)\n",
        "            return additional_epochs\n",
        "        return state['stage2_epochs_trained']\n",
        "    print(\"‚úÖ No previous training found for Stage 2. Starting from epoch 0.\")\n",
        "    return 0\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# üéØ DATA COLLECTION\n",
        "# ===============================\n",
        "\n",
        "\n",
        "print(\"\\n=== COLLECTING DATASET ===\")\n",
        "\n",
        "\n",
        "def collect_datasets_advanced():\n",
        "    classes = []\n",
        "    train_path = data_root\n",
        "\n",
        "\n",
        "    if os.path.exists(train_path):\n",
        "        for item in os.listdir(train_path):\n",
        "            item_path = os.path.join(train_path, item)\n",
        "            if os.path.isdir(item_path):\n",
        "                possible_dirs = ['train', 'training', 'Train', 'Training']\n",
        "                found = False\n",
        "                for sub_dir in possible_dirs:\n",
        "                    train_subdir = os.path.join(item_path, sub_dir)\n",
        "                    if os.path.exists(train_subdir):\n",
        "                        classes.append(item)\n",
        "                        found = True\n",
        "                        break\n",
        "                if not found:\n",
        "                    has_images = any(img.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "                                   for img in os.listdir(item_path))\n",
        "                    if has_images:\n",
        "                        classes.append(item)\n",
        "\n",
        "\n",
        "    classes = sorted(classes)\n",
        "    print(f\"üìã Detected classes: {classes}\")\n",
        "    print(f\"üéØ Total classes: {len(classes)}\")\n",
        "\n",
        "\n",
        "    def collect_split_data_advanced(split):\n",
        "        filepaths = []\n",
        "        labels = []\n",
        "        split_variants = [split, split.lower(), split.upper(), split.capitalize()]\n",
        "\n",
        "\n",
        "        for class_name in classes:\n",
        "            for split_variant in split_variants:\n",
        "                class_dir = os.path.join(data_root, class_name, split_variant)\n",
        "                if os.path.exists(class_dir):\n",
        "                    for img in os.listdir(class_dir):\n",
        "                        if img.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.webp')):\n",
        "                            filepaths.append(os.path.join(class_dir, img))\n",
        "                            labels.append(class_name)\n",
        "                    break\n",
        "\n",
        "\n",
        "            class_dir_direct = os.path.join(data_root, class_name)\n",
        "            if split == 'train' and not any(class_name in label for label in labels):\n",
        "                for img in os.listdir(class_dir_direct):\n",
        "                    if img.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.webp')):\n",
        "                        filepaths.append(os.path.join(class_dir_direct, img))\n",
        "                        labels.append(class_name)\n",
        "\n",
        "\n",
        "        return pd.DataFrame({'filename': filepaths, 'class': labels})\n",
        "\n",
        "\n",
        "    df_train = collect_split_data_advanced('train')\n",
        "    df_val = collect_split_data_advanced('val')\n",
        "    df_test = collect_split_data_advanced('test')\n",
        "\n",
        "\n",
        "    return df_train, df_val, df_test, classes\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "df_train, df_val, df_test, classes = collect_datasets_advanced()\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "print(f\"\\nüìä Dataset Summary:\")\n",
        "print(f\"Train: {len(df_train)} images\")\n",
        "print(f\"Val: {len(df_val)} images\")\n",
        "print(f\"Test: {len(df_test)} images\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ‚öñÔ∏è CLASS BALANCE ANALYSIS\n",
        "# ===============================\n",
        "\n",
        "\n",
        "print(\"\\n=== CLASS BALANCE ANALYSIS ===\")\n",
        "train_class_counts = df_train['class'].value_counts()\n",
        "print(\"Training class distribution:\")\n",
        "for class_name, count in train_class_counts.items():\n",
        "    print(f\"  {class_name}: {count} samples\")\n",
        "\n",
        "\n",
        "# T√≠nh class weights\n",
        "class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(df_train['class']),\n",
        "    y=df_train['class']\n",
        ")\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(classes))}\n",
        "print(\"Class weights:\", class_weight_dict)\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# üöÄ ULTIMATE DATA GENERATOR\n",
        "# ===============================\n",
        "\n",
        "\n",
        "class UltimateDataGenerator(Sequence):\n",
        "    def __init__(self, dataframe, target_size=(300, 300), batch_size=16, shuffle=True, augment=False, mixup_alpha=0.2, cutmix_alpha=1.0):\n",
        "        self.dataframe = dataframe.reset_index(drop=True)\n",
        "        self.target_size = target_size\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.mixup_alpha = mixup_alpha\n",
        "        self.cutmix_alpha = cutmix_alpha\n",
        "        self.classes = classes\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "        self.num_classes = len(self.classes)\n",
        "        self.on_epoch_end()\n",
        "\n",
        "\n",
        "        # Data Augmentation\n",
        "        if self.augment:\n",
        "            self.augmentation_pipeline = ImageDataGenerator(\n",
        "                rotation_range=45,\n",
        "                width_shift_range=0.3,\n",
        "                height_shift_range=0.3,\n",
        "                shear_range=0.3,\n",
        "                zoom_range=0.4,\n",
        "                horizontal_flip=True,\n",
        "                vertical_flip=True,\n",
        "                brightness_range=[0.7, 1.3],\n",
        "                channel_shift_range=0.3,\n",
        "                fill_mode='reflect'\n",
        "            )\n",
        "\n",
        "\n",
        "            self.advanced_augmentations = {\n",
        "                'random_contrast': lambda x: tf.image.random_contrast(x, 0.8, 1.2),\n",
        "                'random_saturation': lambda x: tf.image.random_saturation(x, 0.8, 1.2),\n",
        "                'random_hue': lambda x: tf.image.random_hue(x, 0.1),\n",
        "            }\n",
        "        else:\n",
        "            self.augmentation_pipeline = None\n",
        "            self.advanced_augmentations = None\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.dataframe) / self.batch_size))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batch_data = self.dataframe.iloc[batch_indices]\n",
        "\n",
        "\n",
        "        X = np.zeros((len(batch_data), *self.target_size, 3), dtype=np.float32)\n",
        "        y = np.zeros((len(batch_data), self.num_classes), dtype=np.float32)\n",
        "\n",
        "\n",
        "        for i, (_, row) in enumerate(batch_data.iterrows()):\n",
        "            img = self.load_and_preprocess_image(row['filename'])\n",
        "\n",
        "\n",
        "            if self.augment and self.augmentation_pipeline and np.random.random() > 0.2:\n",
        "                img = self.augmentation_pipeline.random_transform(img.astype(np.float32))\n",
        "\n",
        "\n",
        "                if np.random.random() > 0.5:\n",
        "                    aug_name = np.random.choice(list(self.advanced_augmentations.keys()))\n",
        "                    try:\n",
        "                        img = self.advanced_augmentations[aug_name](img)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "\n",
        "            X[i] = img\n",
        "            y[i, self.class_to_idx[row['class']]] = 1.0\n",
        "\n",
        "\n",
        "        # MixUp Augmentation\n",
        "        if self.augment and self.mixup_alpha > 0 and np.random.random() > 0.7:\n",
        "            X, y = self.mixup(X, y, alpha=self.mixup_alpha)\n",
        "\n",
        "\n",
        "        # CutMix Augmentation\n",
        "        elif self.augment and self.cutmix_alpha > 0 and np.random.random() > 0.7:\n",
        "            X, y = self.cutmix(X, y, alpha=self.cutmix_alpha)\n",
        "\n",
        "\n",
        "        return X, y\n",
        "\n",
        "\n",
        "    def mixup(self, batch_x, batch_y, alpha=0.2):\n",
        "        batch_size = batch_x.shape[0]\n",
        "        indices = np.random.permutation(batch_size)\n",
        "\n",
        "\n",
        "        lam = np.random.beta(alpha, alpha, batch_size)\n",
        "        lam = np.maximum(lam, 1 - lam)\n",
        "        lam = lam.reshape((batch_size, 1, 1, 1))\n",
        "\n",
        "\n",
        "        mixed_x = lam * batch_x + (1 - lam) * batch_x[indices]\n",
        "        mixed_y = lam.reshape((batch_size, 1)) * batch_y + (1 - lam.reshape((batch_size, 1))) * batch_y[indices]\n",
        "\n",
        "\n",
        "        return mixed_x, mixed_y\n",
        "\n",
        "\n",
        "    def cutmix(self, batch_x, batch_y, alpha=1.0):\n",
        "        batch_size, H, W, C = batch_x.shape\n",
        "        indices = np.random.permutation(batch_size)\n",
        "\n",
        "\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "        cut_ratio = np.sqrt(1 - lam)\n",
        "        cut_w = int(W * cut_ratio)\n",
        "        cut_h = int(H * cut_ratio)\n",
        "\n",
        "\n",
        "        cx = np.random.randint(W)\n",
        "        cy = np.random.randint(H)\n",
        "\n",
        "\n",
        "        x1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "        y1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "        x2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "        y2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "\n",
        "        batch_x_copy = batch_x.copy()\n",
        "        batch_x_copy[:, y1:y2, x1:x2, :] = batch_x[indices, y1:y2, x1:x2, :]\n",
        "\n",
        "\n",
        "        lam = 1 - ((x2 - x1) * (y2 - y1) / (W * H))\n",
        "        mixed_y = lam * batch_y + (1 - lam) * batch_y[indices]\n",
        "\n",
        "\n",
        "        return batch_x_copy, mixed_y\n",
        "\n",
        "\n",
        "    def load_and_preprocess_image(self, filepath):\n",
        "        try:\n",
        "            img = cv2.imread(filepath)\n",
        "            if img is None:\n",
        "                return self._create_placeholder_image()\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "            if img.shape[0] > self.target_size[0] or img.shape[1] > self.target_size[1]:\n",
        "                interpolation = cv2.INTER_AREA\n",
        "            else:\n",
        "                interpolation = cv2.INTER_CUBIC\n",
        "\n",
        "\n",
        "            img = cv2.resize(img, self.target_size, interpolation=interpolation)\n",
        "            img = preprocess_input(img.astype(np.float32))\n",
        "\n",
        "\n",
        "            return img\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing {filepath}: {str(e)}\")\n",
        "            return self._create_placeholder_image()\n",
        "\n",
        "\n",
        "    def _create_placeholder_image(self):\n",
        "        return np.random.normal(0, 1, (*self.target_size, 3)).astype(np.float32)\n",
        "\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indices = np.arange(len(self.dataframe))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è DATA PREPARATION\n",
        "# ===============================\n",
        "\n",
        "\n",
        "print(\"\\n=== CREATING DATA GENERATORS ===\")\n",
        "target_size = (300, 300)\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "def fast_clean_dataframe(df):\n",
        "    print(f\"üîç Cleaning {len(df)} images...\")\n",
        "    valid_files = []\n",
        "\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        if idx % 100 == 0:\n",
        "            print(f\"   Progress: {idx}/{len(df)}\")\n",
        "\n",
        "\n",
        "        if os.path.exists(row['filename']):\n",
        "            valid_files.append(idx)\n",
        "\n",
        "\n",
        "    result = df.iloc[valid_files].reset_index(drop=True)\n",
        "    print(f\"‚úÖ Cleaned: {len(result)}/{len(df)} images valid\")\n",
        "    return result\n",
        "\n",
        "\n",
        "# Clean data\n",
        "df_train = fast_clean_dataframe(df_train)\n",
        "df_val = fast_clean_dataframe(df_val)\n",
        "df_test = fast_clean_dataframe(df_test)\n",
        "\n",
        "\n",
        "print(f\"üìä Final Dataset Summary:\")\n",
        "print(f\"Train: {len(df_train)} images\")\n",
        "print(f\"Val: {len(df_val)} images\")\n",
        "print(f\"Test: {len(df_test)} images\")\n",
        "\n",
        "\n",
        "# Create generators\n",
        "train_gen = UltimateDataGenerator(df_train, target_size, batch_size, shuffle=True, augment=True, mixup_alpha=0.2, cutmix_alpha=1.0)\n",
        "val_gen = UltimateDataGenerator(df_val, target_size, batch_size, shuffle=False, augment=False)\n",
        "if len(val_gen) == 0:\n",
        "    print(\"‚ö†Ô∏è Warning: Val generator empty! Splitting from train.\")\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    df_train_split, df_val_split = train_test_split(df_train, test_size=0.2, random_state=42, stratify=df_train['class'])\n",
        "    df_val = df_val_split\n",
        "    df_train = df_train_split\n",
        "    val_gen = UltimateDataGenerator(df_val, target_size, batch_size, shuffle=False, augment=False)\n",
        "test_gen = UltimateDataGenerator(df_test, target_size, batch_size, shuffle=False, augment=False)\n",
        "\n",
        "\n",
        "print(\"‚úÖ Data generators created successfully!\")\n",
        "print(f\"Train batches: {len(train_gen)}\")\n",
        "print(f\"Val batches: {len(val_gen)}\")\n",
        "print(f\"Test batches: {len(test_gen)}\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# üß† ULTIMATE MODEL ARCHITECTURE\n",
        "# ===============================\n",
        "\n",
        "\n",
        "def create_ultimate_model():\n",
        "    base_model = EfficientNetB4(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(*target_size, 3),\n",
        "        pooling='avg'\n",
        "    )\n",
        "    base_model.trainable = False\n",
        "\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        base_model,\n",
        "\n",
        "\n",
        "        # Layer 1\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(1536, activation='relu',\n",
        "                    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)),\n",
        "        layers.BatchNormalization(),\n",
        "\n",
        "\n",
        "        # Layer 2\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(1024, activation='relu',\n",
        "                    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)),\n",
        "        layers.BatchNormalization(),\n",
        "\n",
        "\n",
        "        # Layer 3\n",
        "        layers.Dropout(0.35),\n",
        "        layers.Dense(896, activation='relu',\n",
        "                    kernel_regularizer=regularizers.l2(1e-4)),\n",
        "        layers.BatchNormalization(),\n",
        "\n",
        "\n",
        "        # Layer 4\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(768, activation='relu',\n",
        "                    kernel_regularizer=regularizers.l2(1e-4)),\n",
        "        layers.BatchNormalization(),\n",
        "\n",
        "\n",
        "        # Layer 5\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Dense(512, activation='relu',\n",
        "                    kernel_regularizer=regularizers.l2(1e-4)),\n",
        "        layers.BatchNormalization(),\n",
        "\n",
        "\n",
        "        # Layer 6\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(384, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "\n",
        "\n",
        "        # Layer 7\n",
        "        layers.Dropout(0.15),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "\n",
        "\n",
        "        # Layer 8\n",
        "        layers.Dropout(0.1),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "\n",
        "\n",
        "        # Output\n",
        "        layers.Dense(num_classes, activation='softmax', dtype='float32')\n",
        "    ])\n",
        "\n",
        "\n",
        "    return model, base_model\n",
        "\n",
        "\n",
        "print(\"üöÄ Creating ultimate model...\")\n",
        "model, base_model = create_ultimate_model()\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ‚öôÔ∏è MODEL COMPILATION\n",
        "# ===============================\n",
        "\n",
        "\n",
        "initial_learning_rate = 0.001\n",
        "\n",
        "\n",
        "optimizer = AdamW(\n",
        "    learning_rate=initial_learning_rate,\n",
        "    weight_decay=0.0001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07\n",
        ")\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy',\n",
        "             tf.keras.metrics.Precision(name='precision'),\n",
        "             tf.keras.metrics.Recall(name='recall'),\n",
        "             tf.keras.metrics.AUC(name='auc')]\n",
        ")\n",
        "print(\"‚úÖ Model compiled successfully!\")\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# üìà TRAINING SETUP\n",
        "# ===============================\n",
        "\n",
        "\n",
        "def advanced_lr_schedule(epoch):\n",
        "    if epoch < 5:\n",
        "        return initial_learning_rate * (epoch + 1) / 5\n",
        "    elif epoch < 30:\n",
        "        return initial_learning_rate * 0.5 * (1 + np.cos(np.pi * (epoch - 5) / 25))\n",
        "    elif epoch < 60:\n",
        "        return initial_learning_rate * 0.1 * (1 + np.cos(np.pi * (epoch - 30) / 30))\n",
        "    elif epoch < 90:\n",
        "        return initial_learning_rate * 0.01\n",
        "    else:\n",
        "        return initial_learning_rate * 0.001\n",
        "\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    LearningRateScheduler(advanced_lr_schedule, verbose=1),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_accuracy',\n",
        "        factor=0.5,\n",
        "        patience=6,\n",
        "        min_lr=1e-9,\n",
        "        verbose=1,\n",
        "        mode='max'\n",
        "    ),\n",
        "    EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=35,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "        mode='max'\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        'best_ultimate_model.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    CSVLogger('ultimate_training_log.csv', append=True),\n",
        "    TerminateOnNaN()\n",
        "]\n",
        "\n",
        "\n",
        "print(\"‚úÖ Training callbacks ready!\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# üèãÔ∏è TRAINING EXECUTION\n",
        "# ===============================\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"üî•\" * 60)\n",
        "print(\"üî• STAGE 1: TRAINING HEAD LAYERS\")\n",
        "print(\"üî•\" * 60)\n",
        "\n",
        "\n",
        "print(\"üöÄ Starting Stage 1 training...\")\n",
        "initial_epoch_stage1 = resume_stage1(model)\n",
        "\n",
        "\n",
        "# Update callbacks for stage 1 to save every epoch\n",
        "stage1_checkpoints = ModelCheckpoint(\n",
        "    'stage1_epoch_{epoch:02d}.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=False,  # Save every epoch for resume\n",
        "    save_weights_only=False,  # Save full model for optimizer state\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Th√™m drive backup callback cho m·ªói epoch\n",
        "stage1_backup = drive_backup_callback('stage1')\n",
        "\n",
        "\n",
        "callbacks_stage1 = [\n",
        "    LearningRateScheduler(advanced_lr_schedule, verbose=1),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_accuracy',\n",
        "        factor=0.5,\n",
        "        patience=6,\n",
        "        min_lr=1e-9,\n",
        "        verbose=1,\n",
        "        mode='max'\n",
        "    ),\n",
        "    EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=35,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "        mode='max'\n",
        "    ),\n",
        "    stage1_checkpoints,  # Updated\n",
        "    ModelCheckpoint(  # Keep best only as well\n",
        "        'best_ultimate_model.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    CSVLogger('ultimate_training_log.csv', append=True),\n",
        "    TerminateOnNaN(),\n",
        "    stage1_backup  # Backup m·ªói epoch l√™n Drive\n",
        "]\n",
        "\n",
        "\n",
        "history_stage1 = model.fit(\n",
        "    train_gen,\n",
        "    epochs=100,\n",
        "    initial_epoch=initial_epoch_stage1,\n",
        "    validation_data=val_gen,\n",
        "    callbacks=callbacks_stage1,\n",
        "    verbose=1,\n",
        "    class_weight=class_weight_dict\n",
        ")\n",
        "\n",
        "\n",
        "epochs_trained_stage1 = initial_epoch_stage1 + len(history_stage1.epoch) if hasattr(history_stage1, 'epoch') and history_stage1.epoch else initial_epoch_stage1\n",
        "save_training_state(1, epochs_trained_stage1, max(history_stage1.history.get('val_accuracy', [0])) or 0)\n",
        "\n",
        "\n",
        "# Backup stage 1 files to Drive (final)\n",
        "backup_file_to_drive('ultimate_training_log.csv', 'logs')\n",
        "for cp in glob.glob('stage1_epoch_*.keras'):\n",
        "    backup_file_to_drive(cp, 'stage1_checkpoints')\n",
        "backup_file_to_drive('best_ultimate_model.keras')\n",
        "\n",
        "\n",
        "print(\"üéâ Stage 1 training completed!\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# üîß FINE-TUNING STAGE\n",
        "# ===============================\n",
        "\n",
        "\n",
        "val_accs = history_stage1.history.get('val_accuracy', [])\n",
        "best_stage1_acc = max(val_accs) if val_accs else max(history_stage1.history.get('accuracy', [0.0]))\n",
        "print(f\"\\nüìä Stage 1 Best Accuracy: {best_stage1_acc:.4f}\")\n",
        "if best_stage1_acc >= 0.75:\n",
        "    print(\"üöÄ PROCEEDING TO STAGE 2: FINE-TUNING\")\n",
        "\n",
        "\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers[:150]:\n",
        "        layer.trainable = False\n",
        "\n",
        "\n",
        "    print(f\"üîì Trainable layers: {sum([layer.trainable for layer in base_model.layers])}/{len(base_model.layers)}\")\n",
        "\n",
        "\n",
        "    fine_tune_optimizer = AdamW(\n",
        "        learning_rate=initial_learning_rate * 0.01,\n",
        "        weight_decay=0.00001,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999\n",
        "    )\n",
        "\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=fine_tune_optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy',\n",
        "                tf.keras.metrics.Precision(name='precision'),\n",
        "                tf.keras.metrics.Recall(name='recall'),\n",
        "                tf.keras.metrics.AUC(name='auc')]\n",
        "    )\n",
        "\n",
        "\n",
        "    # Update callbacks for stage 2 to save every epoch\n",
        "    stage2_checkpoints = ModelCheckpoint(\n",
        "        'stage2_epoch_{epoch:02d}.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=False,  # Save every epoch\n",
        "        save_weights_only=False,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "\n",
        "    # Th√™m drive backup callback cho m·ªói epoch\n",
        "    stage2_backup = drive_backup_callback('stage2')\n",
        "\n",
        "\n",
        "    fine_tune_callbacks = [\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_accuracy',\n",
        "            factor=0.5,\n",
        "            patience=8,\n",
        "            min_lr=1e-10,\n",
        "            verbose=1\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=30,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        stage2_checkpoints,  # Updated\n",
        "        ModelCheckpoint(\n",
        "            'best_ultimate_fine_tuned.keras',\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        CSVLogger('ultimate_fine_tuning_log.csv', append=True),\n",
        "        stage2_backup  # Backup m·ªói epoch l√™n Drive\n",
        "    ]\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"üî•\" * 60)\n",
        "    print(\"üî• STAGE 2: ADVANCED FINE-TUNING\")\n",
        "    print(\"üî•\" * 60)\n",
        "\n",
        "\n",
        "    initial_epoch_stage2 = resume_stage2(model)\n",
        "\n",
        "\n",
        "    history_stage2 = model.fit(\n",
        "        train_gen,\n",
        "        epochs=150,\n",
        "        initial_epoch=initial_epoch_stage2,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=fine_tune_callbacks,\n",
        "        verbose=1,\n",
        "        class_weight=class_weight_dict\n",
        "    )\n",
        "\n",
        "\n",
        "    additional_epochs_stage2 = initial_epoch_stage2 + len(history_stage2.epoch) if hasattr(history_stage2, 'epoch') and history_stage2.epoch else initial_epoch_stage2\n",
        "    save_training_state(2, additional_epochs_stage2)\n",
        "\n",
        "\n",
        "    # Backup stage 2 files to Drive (final)\n",
        "    backup_file_to_drive('ultimate_fine_tuning_log.csv', 'logs')\n",
        "    for cp in glob.glob('stage2_epoch_*.keras'):\n",
        "        backup_file_to_drive(cp, 'stage2_checkpoints')\n",
        "    backup_file_to_drive('best_ultimate_fine_tuned.keras')\n",
        "\n",
        "\n",
        "    final_history = history_stage2\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Stage 1 accuracy below threshold. Skipping fine-tuning.\")\n",
        "    final_history = history_stage1\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# üìä EVALUATION & RESULTS\n",
        "# ===============================\n",
        "\n",
        "\n",
        "print(\"\\n=== FINAL EVALUATION ===\")\n",
        "\n",
        "\n",
        "# Standard evaluation\n",
        "test_loss, test_accuracy, test_precision, test_recall, test_auc = model.evaluate(test_gen, verbose=0)\n",
        "print(f\"üéØ TEST ACCURACY: {test_accuracy:.4f}\")\n",
        "print(f\"üéØ TEST PRECISION: {test_precision:.4f}\")\n",
        "print(f\"üéØ TEST RECALL: {test_recall:.4f}\")\n",
        "print(f\"üéØ TEST AUC: {test_auc:.4f}\")\n",
        "\n",
        "\n",
        "# Ensemble prediction\n",
        "def ensemble_predict(generator, model, n_rounds=5):\n",
        "    all_predictions = []\n",
        "\n",
        "\n",
        "    for i in range(n_rounds):\n",
        "        print(f\"üîÑ Ensemble round {i+1}/{n_rounds}\")\n",
        "        generator.on_epoch_end()\n",
        "        pred = model.predict(generator, verbose=0)\n",
        "        all_predictions.append(pred)\n",
        "\n",
        "\n",
        "    return np.mean(all_predictions, axis=0)\n",
        "\n",
        "\n",
        "Y_pred_ensemble = ensemble_predict(test_gen, model)\n",
        "y_pred_ensemble = np.argmax(Y_pred_ensemble, axis=1)\n",
        "\n",
        "\n",
        "# True labels\n",
        "true_labels = []\n",
        "for i in range(len(test_gen)):\n",
        "    _, y_batch = test_gen[i]\n",
        "    true_labels.extend(np.argmax(y_batch, axis=1))\n",
        "true_labels = np.array(true_labels)\n",
        "\n",
        "\n",
        "ensemble_accuracy = np.sum(y_pred_ensemble == true_labels) / len(true_labels)\n",
        "print(f\"üéØ ENSEMBLE ACCURACY: {ensemble_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "final_accuracy = max(test_accuracy, ensemble_accuracy)\n",
        "\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nüìä DETAILED CLASSIFICATION REPORT:\")\n",
        "print(classification_report(true_labels, y_pred_ensemble, target_names=classes, digits=4))\n",
        "\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "cm = confusion_matrix(true_labels, y_pred_ensemble)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd', xticklabels=classes, yticklabels=classes)\n",
        "plt.title(f'Confusion Matrix - Accuracy: {final_accuracy:.4f}', fontsize=16)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Backup evaluation files\n",
        "backup_file_to_drive('confusion_matrix.png', 'evaluation')\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# üíæ SAVE MODEL\n",
        "# ===============================\n",
        "\n",
        "\n",
        "model.save('ULTIMATE_FOOD_RECOGNITION_MODEL.keras')\n",
        "\n",
        "\n",
        "# Save to Google Drive\n",
        "try:\n",
        "    drive_model_path = os.path.join(backup_dir, 'ULTIMATE_FOOD_RECOGNITION_MODEL.keras')\n",
        "    shutil.copy('ULTIMATE_FOOD_RECOGNITION_MODEL.keras', drive_model_path)\n",
        "    print(f\"‚úÖ Model saved to Drive: {drive_model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not save model to Drive: {e}\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# üéâ FINAL RESULTS\n",
        "# ===============================\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"üéâ\" * 40)\n",
        "print(\"üéâ TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "print(\"üéâ\" * 40)\n",
        "\n",
        "\n",
        "print(f\"\\nüìä FINAL PERFORMANCE SUMMARY:\")\n",
        "print(f\"üéØ FINAL ACCURACY: {final_accuracy:.4f}\")\n",
        "print(f\"üéØ ENSEMBLE ACCURACY: {ensemble_accuracy:.4f}\")\n",
        "print(f\"üéØ STANDARD ACCURACY: {test_accuracy:.4f}\")\n",
        "print(f\"üéØ PRECISION: {test_precision:.4f}\")\n",
        "print(f\"üéØ RECALL: {test_recall:.4f}\")\n",
        "print(f\"üéØ AUC: {test_auc:.4f}\")\n",
        "\n",
        "\n",
        "# Performance assessment\n",
        "if final_accuracy >= 0.90:\n",
        "    print(\"\\nüèÜ EXCEPTIONAL PERFORMANCE! üèÜ\")\n",
        "    print(\"üöÄ Model is production-ready!\")\n",
        "elif final_accuracy >= 0.85:\n",
        "    print(\"\\nüéØ EXCELLENT PERFORMANCE!\")\n",
        "    print(\"üöÄ Model is highly accurate!\")\n",
        "elif final_accuracy >= 0.75:\n",
        "    print(\"\\n‚úÖ VERY GOOD PERFORMANCE!\")\n",
        "    print(\"üí™ Model is reliable!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è GOOD PERFORMANCE\")\n",
        "    print(\"üìà Consider further optimization\")\n",
        "\n",
        "\n",
        "print(f\"\\n‚úÖ MODEL SAVED: ULTIMATE_FOOD_RECOGNITION_MODEL.keras\")\n",
        "print(\"üöÄ READY FOR DEPLOYMENT!\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéØ ULTIMATE FOOD RECOGNITION SYSTEM READY!\")\n",
        "print(\"=\"*60)\n",
        "\n"
      ]
    }
  ]
}